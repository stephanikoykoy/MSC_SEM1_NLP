{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Ngram language model lab (unassessed)\n",
    "\n",
    "In this lab you will do 4 exercises building and evaluating ngram language models of the following types:\n",
    "1. A Maximum Likelihood Estimation (MLE) unigram model\n",
    "2. A bigram model with add-one Laplace smoothing\n",
    "3. A bigram model with general additive/Lidstone (add-k) smoothing\n",
    "4. Ngram models with an advanced interpolation technique, Kneser-Ney snoothing (the methods are provided)\n",
    "\n",
    "Before you start the exercises, make sure you run and understand the examples first. Then complete the exercises using the following 3 files with line-separated text to train the bigger language models on:\n",
    "* training data -- \"switchboard_lm_training.txt\"\n",
    "* heldout/development data -- \"switchboard_lm_heldout.txt\"\n",
    "* test data -- \"switchboard_lm_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "LOG_BASE = 2 # all the way through here we will use log base 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful methods for all exercises:\n",
    "def glue_tokens(tokens, order):\n",
    "    \"\"\"A useful way of glueing tokens together for\n",
    "    Kneser Ney smoothing and other smoothing methods\n",
    "    \n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    return '{0}@{1}'.format(order,' '.join(tokens))\n",
    "\n",
    "def unglue_tokens(tokenstring, order):\n",
    "    \"\"\"Ungluing tokens glued by the glue_tokens method\"\"\"\n",
    "    if order == 1:\n",
    "        return [tokenstring.split(\"@\")[1].replace(\" \",\"\")]\n",
    "    return tokenstring.split(\"@\")[1].split(\" \")\n",
    "\n",
    "def tokenize_sentence(sentence, order):\n",
    "    \"\"\"Returns a list of tokens with the correct numbers of initial\n",
    "    and end tags (this is meant ot be used with a non-backoff model!!!)\n",
    "    \n",
    "    :sentence: a string of text\n",
    "    :param: order is the order of the language model\n",
    "        (1 = unigram, 2 = bigram, 3 =trigram etc.)\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()\n",
    "    tokens = ['<s>'] * (order-1) + tokens + ['</s>']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1@john\n",
      "['john']\n"
     ]
    }
   ],
   "source": [
    "#Â see how glue/unglue tokens works:\n",
    "glued = glue_tokens([\"john\"], 1)\n",
    "unglued = unglue_tokens(glued, 1)\n",
    "print(glued)\n",
    "print(unglued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of sentences (corpus) from the example in the lecture slides\n",
    "sentences = [\n",
    "            \"I am Sam\",\n",
    "            \"Sam I am\",\n",
    "            \"I do not like green eggs and ham\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Build a unigram MLE language model from a simple corpus.\n",
    "An MLE unigram model will tell you how likely a word is to occur, estimated from the function of counts:\n",
    "\n",
    "C(w_i)/N\n",
    "\n",
    "where C(w_i) is the number of times the word at position i occurred in the training corpus, and N is the sum of the counts of all words, or, to put it another way, the length of the training corpus.\n",
    "\n",
    "Notice the tokenization method adds a `</s>` at the end but no `<s>` is needed at the beginning of each sentence\n",
    "    because unigrams do not have a context word (we are only concerned with the frequency of single words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized ['I', 'am', 'Sam', '</s>']\n",
      "tokenized ['Sam', 'I', 'am', '</s>']\n",
      "tokenized ['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>']\n",
      "11 different unigrams observed\n",
      "unigram total 17\n"
     ]
    }
   ],
   "source": [
    "unigrams = Counter()\n",
    "for sent in sentences:\n",
    "    words = tokenize_sentence(sent, 1)\n",
    "    print(\"tokenized\", words)\n",
    "    for w in words:\n",
    "        unigrams[w] +=1\n",
    "unigram_total = sum(unigrams.values()) # to get the denominator for unigram probabilities\n",
    "print(len(unigrams), \"different unigrams observed\")\n",
    "print(\"unigram total\", unigram_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'</s>': 3,\n",
       "         'I': 3,\n",
       "         'Sam': 2,\n",
       "         'am': 2,\n",
       "         'and': 1,\n",
       "         'do': 1,\n",
       "         'eggs': 1,\n",
       "         'green': 1,\n",
       "         'ham': 1,\n",
       "         'like': 1,\n",
       "         'not': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we will measure the model's perplexity of those same training sentences. Note in normal practice you would want to do this on different data (as you will do below).\n",
    "\n",
    "Perplexity is equal to 2 to the power of the cross entropy where cross entropy is the negative sum of all log probabilities from the model normalized by the length of the corpus N.\n",
    "\n",
    "Measure the cross entropy and perplexity on each sentence too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Sam', '</s>'] cross entropy: 2.7949815908897615 perplexity: 6.940220937885672\n",
      "['Sam', 'I', 'am', '</s>'] cross entropy: 2.7949815908897615 perplexity: 6.940220937885672\n",
      "['I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>'] cross entropy: 3.7352489522011934 perplexity: 13.317477627933627\n",
      "unigram corpus cross entropy 3.2927701939369913\n",
      "unigram corpus perplexity 9.799921507045037\n"
     ]
    }
   ],
   "source": [
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s \n",
    "for sent in sentences:\n",
    "    # get the unigram model based probability of each sentence\n",
    "    words = tokenize_sentence(sent, 1) # tokenize sentence with the order 1 as the parameter\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for w in words:\n",
    "        prob = unigrams[w]/unigram_total\n",
    "        logprob = log(prob, LOG_BASE)  # the log of the prob to base 2\n",
    "        s += -log(prob, LOG_BASE) # add the neg log prob to s\n",
    "        sent_s += -log(prob, LOG_BASE)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N  # cross entropy total neg log probs/length for this sentence\n",
    "    sent_perplexity = LOG_BASE ** sent_cross_entropy # perplexity for the sentence 2 to the cross-entropy\n",
    "    print(words, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N # cross entropy of corpus total neg log probs/length\n",
    "perplexity = 2 ** cross_entropy  # perplexity for the corpus 2 to the cross-entropy\n",
    "print(\"unigram corpus cross entropy\", cross_entropy)\n",
    "print(\"unigram corpus perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Build a bigram MLE language model from the same corpus\n",
    "A MLE (unsmoothed) bigram model will tell you how likely a word is to occur given the previous word, estimated from the function of counts:\n",
    "\n",
    "C(w_i-1, w_i)/C(w_i-1)\n",
    "\n",
    "where for any pairs of contiguous words w_i-1, w_i, C(w_i-1, w_i) is the number of times the word at position i follows the word at position i-1 in the training corpus, and C(w_i-1) is the number of times the word at position i-1 occurs in the corpus. E.g. for the bigram probability of 'john likes', C(w_i-1, w_i) is the number of times 'john likes' occurs, and C(w_i-1) is how many times 'john' occurs.\n",
    "\n",
    "Notice the tokenization method adds a `</s>` at the end and also one `<s>` for padding at the beginning as we want to count the number of times the word at the beginning of each sentence begins a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'am', 'Sam', '</s>']\n",
      "['<s>', 'Sam', 'I', 'am', '</s>']\n",
      "['<s>', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>']\n",
      "15 different bigrams\n",
      "11 different bigram contexts (and unigrams) observed\n"
     ]
    }
   ],
   "source": [
    "# First get the counts from the training corpus for bigrams without smoothing\n",
    "bigrams = Counter() # a counter for how many times a given bigram sequence w_i-1,w_i occurs\n",
    "bigram_context = Counter() # a counter for how many times each word is used as a context word w_i-1 (so will include the start symbol)\n",
    "order = 2 # order (i.e. n) of the language model- bigram n=2\n",
    "for s in sentences:\n",
    "    words = tokenize_sentence(s, order)  # tokenize sentence with the order 2 as the parameter\n",
    "    print(words)\n",
    "    for i in range(order - 1, len(words)):\n",
    "        context = words[i-order+1:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        bigrams[glue_tokens(ngram, order)] +=1\n",
    "        bigram_context[glue_tokens(context, 1)] += 1\n",
    "print(len(bigrams.keys()), \"different bigrams\")\n",
    "print(len(bigram_context.keys()), \"different bigram contexts (and unigrams) observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a handy function to calculate the probability of an bigram from the counts\n",
    "def prob_bigram_MLE(ngram):\n",
    "    \"\"\"A simple function to compute the \n",
    "    MLE probability estimation based on the counts.\n",
    "    Follows the equation:\n",
    "    C(w_i-1, w_i)/C(w_i-1)\n",
    "    \n",
    "    Dictionaries bigrams and bigram_context are global variables.\n",
    "\n",
    "    \"\"\"\n",
    "    numerator = bigrams[glue_tokens(ngram, 2)]\n",
    "    denominator = bigram_context[glue_tokens(ngram[:1], 1)]\n",
    "    prob = numerator / denominator\n",
    "    return prob\n",
    "\n",
    "\n",
    "if False: #Â optional- see if the continuation probabilities sum to 1\n",
    "    # check if each bigram continuation distribution sums to one\n",
    "    # look at the distributions of possible continuations after each word\n",
    "    for context, v in bigram_context.items():\n",
    "        context = unglue_tokens(context, 1)\n",
    "        print(\"%% context\", context)\n",
    "        check_ngram_total_sums_to_1 = 0\n",
    "        # for a given context the continuation probabilities \n",
    "        # over the whole vocab should sum to 1\n",
    "        for u in unigrams.keys():\n",
    "            ngram = context + [u]\n",
    "            prob = prob_bigram_MLE(ngram)\n",
    "            print(ngram, prob)\n",
    "            check_ngram_total_sums_to_1 += prob\n",
    "        print(\"sums to 1?\", check_ngram_total_sums_to_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'2@<s> I': 2,\n",
       "         '2@<s> Sam': 1,\n",
       "         '2@I am': 2,\n",
       "         '2@I do': 1,\n",
       "         '2@Sam </s>': 1,\n",
       "         '2@Sam I': 1,\n",
       "         '2@am </s>': 1,\n",
       "         '2@am Sam': 1,\n",
       "         '2@and ham': 1,\n",
       "         '2@do not': 1,\n",
       "         '2@eggs and': 1,\n",
       "         '2@green eggs': 1,\n",
       "         '2@ham </s>': 1,\n",
       "         '2@like green': 1,\n",
       "         '2@not like': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "0.5\n",
      "0.5\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Check the estimates for the lecture examples:\n",
    "# p(I|<s>)\n",
    "# p(Sam|<s>)\n",
    "# p(am|I)\n",
    "# p(</s>|Sam)\n",
    "# p(Sam|am)\n",
    "# p(do|I)\n",
    "\n",
    "print(prob_bigram_MLE(['<s>','I']))\n",
    "print(prob_bigram_MLE(['<s>', 'Sam']))\n",
    "print(prob_bigram_MLE(['I', 'am']))\n",
    "print(prob_bigram_MLE(['Sam', '</s>']))\n",
    "print(prob_bigram_MLE(['am', 'Sam']))\n",
    "print(prob_bigram_MLE(['I', 'do']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, as in the unigram case above we will measure the model's perplexity of those same training sentences.\n",
    "\n",
    "Notice that even with this small corpus the bigram perplexity is significantly lower than the unigram perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'am', 'Sam', '</s>'] cross entropy: 0.7924812503605781 perplexity: 1.7320508075688774\n",
      "['<s>', 'Sam', 'I', 'am', '</s>'] cross entropy: 1.042481250360578 perplexity: 2.0597671439071177\n",
      "['<s>', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>'] cross entropy: 0.24110277793803472 perplexity: 1.1818957424692271\n",
      "bigram corpus cross entropy 0.5593985296662904\n",
      "bigram corpus perplexity 1.4736547115524326\n"
     ]
    }
   ],
   "source": [
    "s = 0  # total neg log prob mass for cross entropy\n",
    "N = 0 # total number of words for normalizing s\n",
    "for sent in sentences:\n",
    "    words = tokenize_sentence(sent, order)\n",
    "    sent_s = 0  # recording non-normalized entropy for this sentence\n",
    "    sent_N = 0  # total number of words in this sentence (for normalization)\n",
    "    for i in range(order - 1, len(words)):\n",
    "        context = words[i-order+1:i]\n",
    "        target = words[i]\n",
    "        ngram = context + [target]\n",
    "        prob = prob_bigram_MLE(ngram)\n",
    "        s += -log(prob, LOG_BASE) # add the neg log prob to s\n",
    "        sent_s += -log(prob, LOG_BASE)  # add the neg log prob to sent_s\n",
    "        N += 1 # increment the number of total words\n",
    "        sent_N += 1 # increment the number of total words in this sentence\n",
    "    sent_cross_entropy = sent_s/sent_N  # cross entropy total neg log probs/length for this sentence\n",
    "    sent_perplexity = LOG_BASE ** sent_cross_entropy # perplexity for the sentence 2 to the cross-entropy\n",
    "    print(words, \"cross entropy:\", sent_cross_entropy, \"perplexity:\", sent_perplexity)\n",
    "cross_entropy = s/N # cross entropy of corpus total neg log probs/length\n",
    "perplexity = 2 ** cross_entropy  # perplexity for the corpus 2 to the cross-entropy\n",
    "print(\"bigram corpus cross entropy\", cross_entropy)\n",
    "print(\"bigram corpus perplexity\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Defining Vocabulary, using OOV word token and building a unigram MLE model from a bigger corpus\n",
    "\n",
    "Write code to read in the file `switchboard_lm_train.txt` which has preprocessed text on each line.\n",
    "\n",
    "You will populate a unigram language model based on that data for an MLE estimation using a `Counter` using this data (see Example 1 above as to how this is done for a smaller dataset).\n",
    "\n",
    "Before you do this, you have to define a vocabulary of words using the training data, which you will keep the same for all of the following exercises. In these exercises, the vocabulary is defined by using a Minimum Document Frequency of 2 in the training data. That means the vocab should only contain words which occur at least twice in the training data.\n",
    "\n",
    "Any words not in the vocabulary in the training, heldout and testing data must be replaced with an out-of-vocab symbol `<unk/>` before processing them.\n",
    " \n",
    "Using this model, calculate the perplexity of the ENTIRE test corpus `switchboard_lm_test.txt`- again, remember to replace words unknown by the model with `<unk/>` before getting these measures. See Example 1 as to how this is done for a unigram model on the smaller example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 2. Bigram model with add-one smoothing\n",
    "\n",
    "Change your method for reading in and training a language model from Exercise 1 so it works for bigrams. Use the same methods for identifying and replacing out-of-vocabulary words as you did in Exercise 1 (i.e. use the same vocabulary).\n",
    "\n",
    "However, in testing, rather than just using the raw counts for implementing MLE probabilities you should implement add-one smoothing (see the lecture notes and Jurafsky & Martin Chapter 3/Manning and Schuetze Chapter 6). Remember this involves using the vocabulary size in the denominator of the formula.\n",
    "\n",
    "Obtain the perplexity score on the test data as above for this smoothed bigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3. Bigram model with general additive (Lidstone) add-k smoothing\n",
    "\n",
    "Modify your code from Exercise 2 such that it generalizes beyond adding 1 to all counts, but can add differing amounts k of mass instead, to implement general additive add-k smoothing.\n",
    "\n",
    "On the HELDOUT corpus `switchboard_lm_heldout.txt` experiment with different values of k (e.g. 0.2, 0.4, 0.6, 0.8, though try others if you can) and report the perplexity scores for these different values in a comment. You could also use an optimization algorithm from scipy.optimize to search this efficiently.\n",
    "\n",
    "Once you find the value which gives you the lowest perplexity on the heldout data, use this model to get the perplexity of the test data once and report the scores in a comment. You should be able to get better (lower) perplexity scores than plus-1 smoothing, however make sure the vocabulary used is the same, else it is not a fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 4. Ngram models with Kneser-Ney smoothing\n",
    "\n",
    "Kneser-Ney smoothing is a state-of-the-art technique for smoothing n-gram models.\n",
    "\n",
    "The algorithm is quite complicated, and is implemented for you below for training/getting the appropriate counts  on the training data (`count_ngrams_interpolated_kneser_ney()`). The training process is implemented below for a trigram model, but without doing the appropriate out-of-vocab word replacement as you've done above, using exactly the same vocabulary.\n",
    "\n",
    "The application at test time is done with the method `kneser_ney_ngram_prob()` using the trained Counters, which gives the probability of the model applied to an ngram of a given order, with a given discount.\n",
    "\n",
    "Try to follow how the training works and how the application of the model to ngrams works, and refer to both Section 3.5 in Jurafsky and Martin and the below article on QM plus (pages 7-8 particularly):\n",
    "\n",
    "\"A Bit of Progress in Language Modeling\" (2001) - Joshua T. Goodman\n",
    "\n",
    "In this exercise, you will first modify the training part of the code so it does the replacement of out-of-vocab words as you did in the previous exercises. You do not need to modify the methods below.\n",
    "\n",
    "On the HELDOUT corpus experiment with different orders from trigram upwards (try 3, 4 and 5) and different discount values (e.g. 0.2, 0.4, 0.6, 0.8, though try others if you can) and report the perplexity scores for these different values in a comment. You could also use an optimization algorithm from scipy.optimize to search the different n values and discount values efficiently. \n",
    "\n",
    "Once you find the order and discount values which gives you the lowest perplexity on the heldout data, use this model to get the perplexity of the TEST data once and report the scores in a comment. You should be able to beat (get a lower) perplexity for this compared to excercises 1-3, though make sure you are always using the same vocabulary, else it is not a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneser-Ney smoothing\n",
    "order = 3  # use a trigram\n",
    "discount = 0.8  # use a discount of 0.8\n",
    "\n",
    "\n",
    "# initialize the maps and counts\n",
    "unigram_denominator = 0\n",
    "ngram_numerator_map = Counter() \n",
    "ngram_denominator_map = Counter() \n",
    "ngram_non_zero_map = Counter()\n",
    "\n",
    "\n",
    "def count_ngrams_interpolated_kneser_ney(tokens,\n",
    "                                   order,\n",
    "                                   ngram_numerator_map,\n",
    "                                   ngram_denominator_map,\n",
    "                                   ngram_non_zero_map,\n",
    "                                   unigram_denominator):\n",
    "    \"\"\"Function used in n-gram language model training\n",
    "    to count the n-grams in tokens and also record the\n",
    "    lower order non -ero counts necessary for interpolated Kneser-Ney\n",
    "    smoothing.\n",
    "    \n",
    "    Taken from Goodman 2001 and generalized to arbitrary orders\"\"\"\n",
    "    for i in range(order-1,len(tokens)): # tokens should have a prefix of order - 1\n",
    "        #print(i)\n",
    "        for d in range(order,0,-1): #go through all the different 'n's backwards\n",
    "            if d == 1:\n",
    "                unigram_denominator += 1   # it has been in a context\n",
    "                ngram_numerator_map[glue_tokens(tokens[i],d)] += 1\n",
    "            else:\n",
    "                den_key = glue_tokens(tokens[i-(d-1) : i], d)\n",
    "                num_key = glue_tokens(tokens[i-(d-1) : i+1], d)\n",
    "                # increment the number of times the denominator/context has been seen by 1\n",
    "                ngram_denominator_map[den_key] += 1\n",
    "                # we store the numerator value to check if it's 0\n",
    "                tmp = ngram_numerator_map[num_key]\n",
    "                # we increment the number of times it's been observed as a numerator\n",
    "                ngram_numerator_map[num_key] += 1\n",
    "                # this incrementation of the n-gram count if d < order\n",
    "                # will only happen if the ngram tested at d+1 was unique\n",
    "                if tmp == 0:\n",
    "                    # if this is the first time we see this ngram\n",
    "                    # increment number of types for which its context\n",
    "                    # has been used as a context for any continuation\n",
    "                    ngram_non_zero_map[den_key] += 1\n",
    "                else:\n",
    "                    break \n",
    "                    # if the ngram has already been seen\n",
    "                    # we don't go down to any lower order models\n",
    "                    # because they will have already been counted as types\n",
    "    # return the updated counts and maps\n",
    "    return ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model by populating the maps and counts\n",
    "corpus = open(\"switchboard_lm_train.txt\")\n",
    "for line in corpus:\n",
    "    tokens = tokenize_sentence(line, order)\n",
    "    ngram_numerator_map, ngram_denominator_map, ngram_non_zero_map, unigram_denominator =\\\n",
    "            count_ngrams_interpolated_kneser_ney(tokens,\n",
    "                                           order,\n",
    "                                           ngram_numerator_map,\n",
    "                                           ngram_denominator_map,\n",
    "                                           ngram_non_zero_map,\n",
    "                                           unigram_denominator)\n",
    "corpus.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kneser_ney_ngram_prob(ngram, discount, order):\n",
    "    \"\"\"KN smoothed ngram probability from Goodman 2001.\n",
    "    This is run at test time to calculate the probability\n",
    "    of a given n-gram or a given order with a given discount.\n",
    "    \n",
    "    ngram :: list of strings, the ngram\n",
    "    discount :: float, the discount used\n",
    "    order :: int, order of the model\n",
    "    \"\"\"\n",
    "    # First, calculate the unigram continuation prob of the last token\n",
    "    # If we've never seen it at all, it will \n",
    "    # have no probability as a numerator\n",
    "    uni_num = ngram_numerator_map.get(glue_tokens(ngram[-1], 1)) # number of bigrams the word is a continuation for\n",
    "    if not uni_num: # if no value found in dict, make it 0\n",
    "        uni_num = 0\n",
    "    # unigram_denominator is the number of different bigram types (not tokens)\n",
    "    probability = previous_prob = uni_num / unigram_denominator\n",
    "    \n",
    "    # Check: Given <unk/> should have been used in place of unknown words before passing\n",
    "    # to this method, probability should be non-zero\n",
    "    if probability == 0.0:\n",
    "        print(\"0 prob for unigram!\")\n",
    "        print(glue_tokens(ngram[-1], 1))\n",
    "        print(ngram)\n",
    "        print(ngram_numerator_map.get(glue_tokens(ngram[-1], 1)))\n",
    "        print(unigram_denominator)\n",
    "        raise Exception\n",
    "\n",
    "    # Compute the higher order probs (from 2/bi-gram upwards) and interpolate them\n",
    "    for d in range(2,order+1):\n",
    "        # Get the context count for the denominator:\n",
    "        # When d = order (n) this is the number of times it's observed in the corpus (tokens)\n",
    "        # When d < order (n) this is the number of different continuation types (not tokens) seen with it as its prefix\n",
    "        ngram_denom = ngram_denominator_map.get(glue_tokens(ngram[-(d):-1], d))\n",
    "        if not ngram_denom: # if no value found in dict, make it 0\n",
    "            ngram_denom = 0\n",
    "        if ngram_denom != 0:\n",
    "            # Get the ngram count for the numerator\n",
    "            # When d = order (n) this is the number of times it's observed in the corpus (tokens)\n",
    "            # When d < order (n) this is the number of types of unique ngram for n=d+1 it is a continuation for (types)\n",
    "            ngram_num = ngram_numerator_map.get(glue_tokens(ngram[-(d):], d))\n",
    "            if not ngram_num: # if no value found in dict, make it 0\n",
    "                ngram_num = 0\n",
    "            if ngram_num != 0:\n",
    "                # calculate the prob with fixed discount\n",
    "                current_prob = (ngram_num - discount) / ngram_denom\n",
    "            else:\n",
    "                current_prob = 0.0\n",
    "            # get the number of word types that can follow the context\n",
    "            # (number of times normalised discount has been applied):\n",
    "            nonzero = ngram_non_zero_map.get(glue_tokens(ngram[-(d):-1], d))\n",
    "            if not nonzero: # if no value found in dict, make it 0\n",
    "                nonzero = 0\n",
    "            #Â get the lambda for this context\n",
    "            lambda_context = discount / ngram_denom * nonzero\n",
    "            # interpolate with previous probability of lower orders calculated so far\n",
    "            current_prob += lambda_context * previous_prob\n",
    "            previous_prob = current_prob\n",
    "            probability = current_prob\n",
    "        else:\n",
    "            #if this context (e.g. bigram context for trigrams) has never been seen, \n",
    "            #then we can only use the last order with a probability (e.g. unigram)\n",
    "            #and halt\n",
    "            probability = previous_prob\n",
    "            break\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
